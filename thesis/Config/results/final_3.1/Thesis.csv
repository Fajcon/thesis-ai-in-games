Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
10000,9.703837,0.008606926,998.625,0.7503752298653126,0.7503752298653126,1.0
20000,9.703835,0.005549224,999.0,8.443991343180339e-08,8.443991343180339e-08,1.0
30000,9.695135,0.009324747,999.0,0.3750002086162567,0.3750002086162567,1.0
40000,9.694546,0.013132421,999.0,0.5833335494001707,0.5833335494001707,1.0
50000,9.6845255,0.0059028957,999.0,-0.12499991059303284,-0.12499991059303284,1.0
60000,9.683116,0.022261852,999.0,0.25000013411045074,0.25000013411045074,1.0
70000,9.671773,0.00922176,999.0,0.7500001899898052,0.7500001899898052,1.0
80000,9.660847,-0.002160049,999.0,-0.08333321164051692,-0.08333321164051692,1.0
90000,9.652677,0.013871543,999.0,1.1250003017485142,1.1250003017485142,1.0
100000,9.6481495,0.023571037,999.0,0.583333524564902,0.583333524564902,1.0
110000,9.649279,0.017985184,999.0,0.25000014528632164,0.25000014528632164,1.0
120000,9.647588,0.016043209,999.0,0.6666668901840845,0.6666668901840845,1.0
130000,9.64488,0.019075943,999.0,0.62500024959445,0.62500024959445,1.0
140000,9.651217,0.038175736,999.0,1.0000003377596538,1.0000003377596538,1.0
150000,9.650865,0.04205248,999.0,0.87500024959445,0.87500024959445,1.0
160000,9.653297,0.065330006,999.0,1.416666900118192,1.416666900118192,1.0
170000,9.649984,0.066938296,999.0,0.7500002644956112,0.7500002644956112,1.0
